#!/usr/bin/env python3
"""
Model Verification Script - Compare PyTorch and CoreML outputs
Uses existing conversion scripts to test model equivalence
"""

import os
import sys
import subprocess
import tempfile
import numpy as np
from pathlib import Path

def run_command(cmd, cwd=None):
    """Run command and return result"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=cwd)
        return result.returncode == 0, result.stdout, result.stderr
    except Exception as e:
        return False, "", str(e)

def verify_model_with_test_script(model_name):
    """Verify a model using the existing conversion test script"""

    print(f"\nüîç Verifying {model_name}")
    print("=" * 40)

    # Run the simple test conversion
    success, stdout, stderr = run_command(f"python scripts/convert_simple_test.py")

    if success:
        print("‚úÖ Basic conversion pipeline test passed")
        return True
    else:
        print(f"‚ùå Basic conversion test failed: {stderr}")
        return False

def verify_coreml_model_loading(model_name):
    """Test if CoreML model can be loaded and basic info retrieved"""

    model_path = f"models/{model_name}.mlpackage"
    if not os.path.exists(model_path):
        model_path = f"models/{model_name}.mlmodel"

    if not os.path.exists(model_path):
        print(f"‚ùå CoreML model not found: {model_path}")
        return False

    try:
        import coremltools as ct
        model = ct.models.MLModel(model_path)

        # Get model spec
        spec = model.get_spec()

        print(f"‚úÖ CoreML model loaded successfully")
        print(f"   Input description: {spec.description.input[0].name}")
        print(f"   Output description: {spec.description.output[0].name}")

        # Check if model has metadata
        if hasattr(spec, 'metadata') and spec.metadata.userDefined:
            print(f"   Metadata: {len(spec.metadata.userDefined)} fields")

        return True

    except Exception as e:
        print(f"‚ùå Failed to load CoreML model: {e}")
        return False

def verify_pytorch_weights(model_name):
    """Test if PyTorch weights can be loaded"""

    # Try different file extensions
    weight_paths = [
        f"weights/{model_name}.pth",
        f"weights/{model_name}.pt",
        f"weights/{model_name}.npz"
    ]

    weights_found = False
    for weight_path in weight_paths:
        if os.path.exists(weight_path):
            weights_found = True
            try:
                import torch
                weights = torch.load(weight_path, map_location='cpu')
                print(f"‚úÖ PyTorch weights loaded: {weight_path}")

                if isinstance(weights, dict):
                    print(f"   Weight keys: {list(weights.keys())[:3]}...")  # Show first 3 keys
                    if 'params' in weights or 'params_ema' in weights:
                        print("   ‚úÖ Real-ESRGAN format detected")

                return True

            except Exception as e:
                print(f"‚ùå Failed to load PyTorch weights {weight_path}: {e}")
                return False

    if not weights_found:
        print(f"‚ö†Ô∏è  No PyTorch weights found for {model_name}")
        # This is ok for generated models
        return True

    return False

def run_pytorch_vs_coreml_test():
    """Compare PyTorch and CoreML model outputs directly"""

    print(f"\nüî¨ Running PyTorch vs CoreML comparison")
    print("=" * 40)

    try:
        import torch
        import coremltools as ct
        import numpy as np
        # Get device without importing inference_pipeline
        if torch.cuda.is_available():
            device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device('mps')
        else:
            device = torch.device('cpu')
        print(f"‚úÖ Using device: {device}")

        # Test with available models
        models_dir = Path("models")
        tested_models = 0
        successful_comparisons = 0

        # Find test models (start with lightweight ones)
        test_models = ["SRCNN_x2", "SRCNN_x3", "ESRGAN_2x", "ESRGAN_4x"]

        for model_name in test_models:
            # Check if CoreML model exists
            coreml_path = None
            for ext in ['.mlpackage', '.mlmodel']:
                path = models_dir / f"{model_name}{ext}"
                if path.exists():
                    coreml_path = path
                    break

            if not coreml_path:
                print(f"‚ö†Ô∏è  CoreML model not found: {model_name}")
                continue

            # Check if PyTorch weights exist
            weight_path = None
            for ext in ['.pth', '.pt', '.npz']:
                path = Path(f"weights/{model_name}{ext}")
                if path.exists():
                    weight_path = path
                    break

            if not weight_path:
                print(f"‚ö†Ô∏è  PyTorch weights not found: {model_name}")
                continue

            print(f"\nüß™ Testing {model_name}")
            print("-" * 30)

            try:
                # Load CoreML model
                coreml_model = ct.models.MLModel(str(coreml_path))
                print(f"‚úÖ CoreML model loaded")

                # Load PyTorch model (create lightweight one if no weights)
                pytorch_model = create_pytorch_model_for_testing(model_name, weight_path)
                pytorch_model.eval()
                print(f"‚úÖ PyTorch model loaded")

                # Create test input
                test_input = torch.randn(1, 3, 64, 64)
                print(f"‚úÖ Test input created: {test_input.shape}")

                # Run PyTorch inference
                with torch.no_grad():
                    pytorch_output = pytorch_model(test_input)
                print(f"‚úÖ PyTorch inference complete: {pytorch_output.shape}")

                # Run CoreML inference
                input_dict = {"input": test_input.numpy()}
                coreml_output_dict = coreml_model.predict(input_dict)
                coreml_output = list(coreml_output_dict.values())[0]
                print(f"‚úÖ CoreML inference complete: {coreml_output.shape}")

                # Compare outputs
                if compare_outputs(pytorch_output, coreml_output, model_name):
                    successful_comparisons += 1

                tested_models += 1

            except Exception as e:
                print(f"‚ùå Failed to test {model_name}: {e}")
                continue

        print(f"\nüìä Model Comparison Results:")
        print(f"   Models tested: {tested_models}")
        print(f"   Successful comparisons: {successful_comparisons}")

        return tested_models > 0 and successful_comparisons == tested_models

    except Exception as e:
        print(f"‚ùå Model comparison failed: {e}")
        return False

def create_pytorch_model_for_testing(model_name, weight_path):
    """Create or load a PyTorch model for testing"""
    import torch
    import torch.nn as nn

    # Try to load from conversion scripts
    try:
        if "SRCNN" in model_name:
            from convert_srcnn import create_srcnn_model
            scale = 2 if "x2" in model_name else 3
            model = create_srcnn_model(scale)
            if weight_path.exists():
                try:
                    state_dict = torch.load(weight_path, map_location='cpu', weights_only=True)
                    model.load_state_dict(state_dict, strict=False)
                except:
                    pass  # Use generated weights
            return model

        elif "ESRGAN" in model_name:
            from convert_esrgan import create_esrgan_model
            scale = 2 if "x2" in model_name else 4
            model = create_esrgan_model(scale, num_in_ch=3, num_out_ch=3)
            if weight_path.exists():
                try:
                    state_dict = torch.load(weight_path, map_location='cpu', weights_only=True)
                    model.load_state_dict(state_dict, strict=False)
                except:
                    pass  # Use generated weights
            return model

    except ImportError:
        pass

    # Fallback: create a simple test model
    class SimpleTestModel(nn.Module):
        def __init__(self, scale_factor=2):
            super().__init__()
            self.scale_factor = scale_factor
            self.conv = nn.Conv2d(3, 3, 3, padding=1)

        def forward(self, x):
            x = self.conv(x)
            x = torch.nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest')
            return x

    scale = 2 if "x2" in model_name else (3 if "x3" in model_name else 4)
    return SimpleTestModel(scale)

def compare_outputs(pytorch_output, coreml_output, model_name):
    """Compare PyTorch and CoreML outputs"""
    import numpy as np

    try:
        # Convert to numpy
        pytorch_np = pytorch_output.detach().cpu().numpy()

        # Check shapes
        if pytorch_np.shape != coreml_output.shape:
            print(f"‚ùå Shape mismatch: PyTorch {pytorch_np.shape} vs CoreML {coreml_output.shape}")
            return False

        # Calculate differences
        abs_diff = np.abs(pytorch_np - coreml_output)
        max_diff = np.max(abs_diff)
        mean_diff = np.mean(abs_diff)

        print(f"üìä Max difference: {max_diff:.6f}")
        print(f"üìä Mean difference: {mean_diff:.6f}")

        # Consider it successful if difference is reasonable
        tolerance = 1e-2  # Relaxed tolerance for generated models
        success = max_diff < tolerance

        if success:
            print(f"‚úÖ {model_name}: Outputs match within tolerance")
        else:
            print(f"‚ö†Ô∏è  {model_name}: Outputs differ significantly")

        return success

    except Exception as e:
        print(f"‚ùå Error comparing outputs: {e}")
        return False

def verify_model_consistency():
    """Check that models and weights are consistent"""

    print(f"\nüìä Checking model consistency")
    print("=" * 40)

    models_dir = Path("models")
    weights_dir = Path("weights")

    if not models_dir.exists():
        print("‚ùå Models directory not found")
        return False

    if not weights_dir.exists():
        print("‚ùå Weights directory not found")
        return False

    # Count models and weights
    model_files = list(models_dir.glob("*.mlpackage")) + list(models_dir.glob("*.mlmodel"))
    weight_files = list(weights_dir.glob("*.pth")) + list(weights_dir.glob("*.pt")) + list(weights_dir.glob("*.npz"))

    print(f"üì¶ Found {len(model_files)} CoreML models")
    print(f"üì¶ Found {len(weight_files)} PyTorch weights")

    # Check for basic models that should always exist
    required_models = ["SRCNN_x2", "SRCNN_x3"]
    missing_models = []

    for model_name in required_models:
        model_path = models_dir / f"{model_name}.mlpackage"
        if not model_path.exists():
            model_path = models_dir / f"{model_name}.mlmodel"

        if not model_path.exists():
            missing_models.append(model_name)

    if missing_models:
        print(f"‚ö†Ô∏è  Missing basic models: {missing_models}")
    else:
        print("‚úÖ All basic models present")

    return len(missing_models) == 0

def main():
    """Main verification function"""

    print("üîç Model Verification & Comparison")
    print("=" * 50)

    # Change to project directory if needed
    script_dir = Path(__file__).parent
    project_dir = script_dir.parent
    os.chdir(project_dir)

    verification_results = []

    # 1. Test basic conversion pipeline
    print("\n1Ô∏è‚É£ Testing basic conversion pipeline")
    result = verify_model_with_test_script("ConversionPipeline")
    verification_results.append(("Basic Conversion", result))

    # 2. Check model consistency
    print("\n2Ô∏è‚É£ Checking model consistency")
    result = verify_model_consistency()
    verification_results.append(("Model Consistency", result))

    # 3. Test CoreML model loading
    print("\n3Ô∏è‚É£ Testing CoreML model loading")
    coreml_success = 0
    coreml_total = 0

    models_dir = Path("models")
    if models_dir.exists():
        model_files = list(models_dir.glob("*.mlpackage")) + list(models_dir.glob("*.mlmodel"))
        # Test first few models to avoid too much output
        for model_file in model_files[:5]:
            model_name = model_file.stem
            coreml_total += 1
            if verify_coreml_model_loading(model_name):
                coreml_success += 1

        print(f"üìä CoreML loading: {coreml_success}/{coreml_total} models successful")
        verification_results.append(("CoreML Loading", coreml_success > 0))

    # 4. Test PyTorch weights loading
    print("\n4Ô∏è‚É£ Testing PyTorch weights loading")
    pytorch_success = 0
    pytorch_total = 0

    weights_dir = Path("weights")
    if weights_dir.exists():
        weight_files = list(weights_dir.glob("*.pth")) + list(weights_dir.glob("*.pt")) + list(weights_dir.glob("*.npz"))
        # Test first few weights
        for weight_file in weight_files[:5]:
            model_name = weight_file.stem
            pytorch_total += 1
            if verify_pytorch_weights(model_name):
                pytorch_success += 1

        print(f"üìä PyTorch loading: {pytorch_success}/{pytorch_total} weights successful")
        verification_results.append(("PyTorch Loading", pytorch_success > 0))

    # 5. Run PyTorch vs CoreML comparison
    print("\n5Ô∏è‚É£ Running PyTorch vs CoreML comparison")
    result = run_pytorch_vs_coreml_test()
    verification_results.append(("PyTorch vs CoreML", result))

    # Summary
    print("\nüìã Verification Summary")
    print("=" * 40)

    passed = 0
    total = len(verification_results)

    for test_name, result in verification_results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name:.<30} {status}")
        if result:
            passed += 1

    print(f"\nüéØ Overall: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All verification tests passed!")
        return True
    else:
        print("‚ö†Ô∏è  Some verification tests failed")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)